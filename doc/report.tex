\documentclass{article}


\usepackage[backend=bibtex]{biblatex}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{siunitx}
\usepackage[table]{xcolor}


\bibliography{report}
\title{Classifying rocks via 7810-dimensional spectroscopy}
\author{Miroslav Vitkov}


\begin{document}
\maketitle


\begin{abstract}
Several machine learning models in various C++ libraries are tried.
The most promising one is then tuned - kernel type, kernel parameters, data preprocessing.
The business application accuracy is theoretically derived from the per sample accuracy.
\end{abstract}


\section{Dataset}
Laser-induced breakdown spectroscopy\cite{libs_intro} involves turning some matter into plasma and observing it's radiant spectrum.
That is measured by a spectrometer and discretized into 7810 buckets of central frequency from  \SI{180}{\nano\metre} upward in steps of \SI{0.1}{\nano\metre}.
The spectrometer has an apparent peak in sensitivity around \SI{440}{\nano\metre} but important spectral lines are spread through the whole measurement range.
\par
The 7810 measured values for each spectrum represent the radiance\cite{radiance} of the plasma convoluted with the \textbf{unknown} impulse response of the measurement equipment.
\par
The dataset consists of 2362 datapoints all classified within 6 labels and 45 sublabels( measurement neighbourhoods ).
The reference number of the dataset is 190401.
The excitation laser wavelength is \SI{1064}{\nano\metre}.


\section{Preprocessing}
\subsection{Noise}
Boiling plasma is not theoretically expected to absorb light of any wavelenght.
The dataset violates this assumpmpion!
\par
\begin{verbatim}
Dataset consists of 2710 files.
The global micro average intensity is 114.398.
The global count of negative values is 2442125, which is 0.115385 of all datapoints.
The mean of all negative values is -9.10135.
The most extreme negative value is -1042.48.
\end{verbatim}
\begin{itemize}
\item{One model of the noise is harmonics of the exciting laser.
This hypothesys fails due to it operating at \SI{1064}{\nano\metre}, which is not a multiple of the observed spikes' frequency.}
\item{Another model is Gaussian noise with $\mu=\SI{440}{\nano\metre}, \sigma=\SI{40}{\nano\metre}$.}
\item{Another model is uniform noise, multiplied by the impulse response of the measurement equipment.
Instrument IR can be recovered by averaging each ferquency's mean value over all sampls (i.e. stochastic mutidimensional mean).}
\end{itemize}
\begin{figure}
\caption{Most extreme negative intensity}
\centering
\includegraphics[width=1.25\textwidth]{img/negatives}
\end{figure}
Noise filtering has not been implemented.


\subsection{Normalization}
Surprizingly worsened SVM performance.
TODO: quote results


\subsection{Feature selection}
TODO: PCA is applied in an attempt to automatically identify the important spectral lines.


\subsection{Dimensionality Reduction}
TODO: For comparisson, a t-distributed stochastic neighbor embedding is ran.


\section{Models}
The following algorithms were tried with default parameters from the corresponding libraries implementing them.
\\ \par
\rowcolors{1}{white}{lightgray}
\begin{tabular}{ c | c | c | c | c }
algorithm      & macro averaged accuracy & prediction time  & training time                    & computational model \\
random chance  & 0.1609                  & \SI{0}{\second}  & \SI{0}{\second}                  & single thread \\
correlation    & 0.8678                  & \SI{16}{\second} & \SI{0}{\second}                  & multithreaded \\
SVM            & 0.9828                  & \SI{0}{\second}  & \SI{3}{\minute} \SI{27}{\second} & GPU \\
neural network & 0.2098                  & \SI{0}{\second}  & \SI{2}{\minute} \SI{7}{\second}  & GPU \\
random forest  & 0.1695                  & \SI{21}{\second} & \SI{1}{\hour} \SI{49}{\minute}   & multithreaded \\
\end{tabular}


\subsection{Random Chance}
For sanity checking of the system, a random chance model was developed.
It looks only at the distribution of training labels and produces a similar stochastic distribution at prediction time.


\subsection{Correlation}
With such a small dataset, correlation is feasible.
It yields good results, but is not scalable.


\subsection{SVM}
A multiclass SVM is tested with a linear kernel because that is the only one dlib supports.

%maximum-margin classifier
%kernel trick
%hard/soft margin
%Radial basis function kernel
%http://webspace.ulbsibiu.ro/lucian.vintan/html/sci.pdf
%one vs one / one vs many
%curse of dimensionality
%https://stats.stackexchange.com/questions/77876/why-would-scaling-features-decrease-svm-performance?rq=1
%https://arxiv.org/pdf/0810.4752v1.pdf
%https://stats.stackexchange.com/questions/10423/number-of-features-vs-number-of-observations/10426#10426
%https://stats.stackexchange.com/a/10426/208261
%https://stats.stackexchange.com/questions/186184/does-dimensionality-curse-effect-some-models-more-than-others
%choice of regularization coefficient c
%choice of kernel
%choice of kernel parameters
% probabilistic classifier


\subsection{Neural Network}
A naive model with two hidden layers with relu activations performed extremely poorly.
It is unclear if this is due to programming errors or due to wrong architecture of the network.


\subsection{Random Forest}
An OpenMP-based random forest classifier took extremely long to train and yielded poor results.


\section{Tuning the model}
The SVM model performs well and runs quickly at prediction time.
This model is selected for parameter tuning.
However, the implementation allows the use of only a linar kernel (why)?
A grid search for the regularization coefficient c in the range [1e-5, 1e3] indicates 1e-4 as best performing.
TODO: more details


\section{Implementation}
\subsection{Language}
C++ was chosen as the language of the implementation.


\subsection{Design}
The program ues the \textbf{Command} design pattern to fit different working modes into one executable.

The classifier models follow the Open/Close OOP principle.
They should have worked with either raw spectra or with preprocessed data, but that would have complicated the interface desing.
Thus preprocessing techniques were tested in hardcoded SVMandX models.

To separate interface from implementation \textbf{pImpl} is used.
Not only do implementation details remain hidden, external includes also remain oly in the .cpp file.

The \textbf{Factory} model::create() connects seamlessly with the command line parser module.

\subsection{Reproducability}
Probably due to programming errors on our part, the PCA and SVM models produce slightly different results from run to run.
The presented results are averaged over several runs.

\subsection{Libraries}
\begin{itemize}
\item{dlib - a small mathematical library, used for correlation, neural networks, SVM.}
\item{Qwt - a native plottng library. It's main rival are the C++ bindings for pyplot.}
\item{OpenCV - used for PCA, because dlib's methods only work for binary classification out of the box.}
\end{itemize}

\subsection{Data structures}
Traditionally a dataframe is used to store and represent a dataset.
To the contrary we used a map from labels to a set of observations.
A function was provided for walking this structure.
Overall this setup performs well except when the shape of the dataframe needs to change.


\section{Implications}
Majority voting with n=60 samples and accuracy a=0.9828 yields overall expected error rate of
\begin{verbatim}
$ find . -name *.csv | wc -l
2710
$ calc "2710 / 45"
~60.22222
>>> import math
>>> # Probability of majority voting to fail:
>>> # ( 1 - 0.9828 ) ^ 30 + ( 1 - 0.9828 ) ^ 31 + ... + ( 1 - 0.9828 ) ^ 60
>>> # Let's take only the largest term.
>>> math.pow(( 1 - 0.9828 ),30)
1.163733153901975e-53
>>> # Let's assume one test per second.
>>> # The Mean Time Between Failures is the invese of the frequency of failures.
>>> 1 / 1.163733153901975e-53
8.593035238766027e+52  # seconds
>>> # Let's divide this by the age of the universe.
>>> 8.593035238766027e+52 / ( 31557600 * 13.8e9 )
1.973165617645385e+35
\end{verbatim}


\section{Conclusion}
\subsection{Experimental}
A large number of test points which are guarangteed to be of the same label is provided during real world operation.
This results in even a simple model like an SVM solving the problem practiacally perfectly.

\subsection{Language}
Customarily machine learning is done in Python.
Those are our observations over the suitability of C++ for such tasks.
\begin{itemize}
\item{Central repository - Python "comes with the batteries included".
                           Somewhat mitigating this, this project uses a pletora of libraries known to be open-source, with persmissive license, easy to use, efficient.
                           The project thus can be used as a demo of C++ ML libraries.
}
\item{Type safety - due to the nature of matrix multiplication, the result is rarely in the same form as the input.
                    Thus new types need to be created between any two operations, making pipelining more tedious.
                    Overall little is gained by labling lists of numbers which change their representation but preserve their meaning.}
\item{Plotting - plotting libraries for C++ are scarce, one of the moest popular candidates being the bindings to matplotlib.}
\item{Dataframes - matrix calculations, consistency between test run results.}
\end{itemize}


\printbibliography


\end{document}
