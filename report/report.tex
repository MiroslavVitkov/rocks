\documentclass{article}


\usepackage[backend=bibtex]{biblatex}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{siunitx}
\usepackage[table]{xcolor}


\bibliography{report}
\title{Classifying rocks via 7810-dimensional spectroscopy}
\author{Miroslav Vitkov}


\begin{document}
\maketitle


\begin{abstract}
Several machine learning models in various C++ libraries are tried.
The most promising one is then tuned - kernel type, kernel parameters, data preprocessing.
The business application accuracy is theoretically derived from the per sample accuracy.
\end{abstract}


\section{Dataset}
Laser-induced breakdown spectroscopy\cite{libs_intro} involves turning some matter into plasma and observing it's radiant spectrum.
That is measured by a spectrometer and discretized into 7810 bins of central frequency from  \SI{180}{\nano\metre} upward in steps of \SI{0.1}{\nano\metre}.
The spectrometer has an apparent peak in sensitivity around \SI{440}{\nano\metre} but important spectral lines are spread through the whole measurement range.
\par
The 7810 measured values for each spectrum represent the radiance\cite{radiance} of the plasma convoluted with the \textbf{unknown} impulse response of the measurement equipment.
\par
The dataset consists of 2362 datapoints all classified within the 6 labels and 45 sublabels(regions).
The reference number of the dataset is 190401.
The excitation laser wavelength is \SI{1064}{\nano\metre}.

\section{Preprocessing}
Boiling plasma is not theoretically expected to absorb light of any wavelenght.
The dataset violates this assumpmpion!
\par
\begin{verbatim}
Dataset consists of 2710 files.
The global micro average intensity is 114.398.
The global count of negative values is 2442125, which is 0.115385 of all datapoints.
The mean of all negative values is -9.10135.
The most extreme negative value is -1042.48.
\end{verbatim}
\par
One model of the noise is harmonics of the exciting laser.
This  hypothesys fails due to it operating at \SI{1064}{\nano\metre}.
\par
Another model is Gaussian noise with $\mu=\SI{440}{\nano\metre}, \sigma=\SI{40}{\nano\metre}$.
It has not been tested.
\par
Another model is uniform noise, multiplied by the impulse response of the measurement equipment.
This should be trivial to implement, with each ferquency's mean value being the mean of all sampls (i.e. stochastic mean).

\begin{figure}
\caption{Most extreme negative intensity}
\centering
\includegraphics[width=1.25\textwidth]{img/negatives}
\end{figure}
\par
Other considered but unimplemented models of the noice include:
\begin{itemize}
\item{kur}
\end{itemize}

\section{Models}
The following algorithms were tried with default parameters from the corresponding libraries implementing them.
Accuracy is calculated as:
Prediction time is:
Training time is:
Computational mode illustrates how much room for improvement there is in the used implementation.
\\ \par
\rowcolors{1}{white}{lightgray}
\begin{tabular}{ c | c | c | c | c }
algorithm      & accuracy & prediction time & training time & computational model \\
random chance  & & & & single thread \\
correlation    & & & & multithreaded \\
SVM            & & & & GPU \\ verify!!
neural network & & & & GPU \\  verify!!
random forest  & & & & multithreaded \\
\end{tabular}


\subsection{Random Chance}
Firstly, for sanity checking of the system, a random chance model was developed.
It looks only at the distribution of training labels and produces a similar stochastic distribution at prediction time.


\section{Correlation}
With such a small dataset, correlation is feasible.
It yields good results, but is not scalable.


\section{SVM}
The SVM model performs well and runs quickly at prediction time.
%maximum-margin classifier
%kernel trick
%hard/soft margin
%Radial basis function kernel
%http://webspace.ulbsibiu.ro/lucian.vintan/html/sci.pdf
%one vs one / one vs many
%curse of dimensionality
%https://stats.stackexchange.com/questions/77876/why-would-scaling-features-decrease-svm-performance?rq=1
%https://arxiv.org/pdf/0810.4752v1.pdf
%https://stats.stackexchange.com/questions/10423/number-of-features-vs-number-of-observations/10426#10426
%https://stats.stackexchange.com/a/10426/208261
%https://stats.stackexchange.com/questions/186184/does-dimensionality-curse-effect-some-models-more-than-others
%choice of regularization coefficient c
%choice of kernel
%choice of kernel parameters
% probabilistic classifier

\section{Neural Network}
A naive model with two hidden layers with relu activations performed extremely poorly: <>.
It is unclear if this is due to programming errors or due to wrong architecture of the network.


\section{Random Forest}
A OpenMP-based random forest classifier took extremely long to train and yielded the following result:


\section{Conclusion}
An SVM is a suitable baseline model for multiclass classification of high dimensionality objects.


%\bibliographystyle{te}
\printbibliography


\end{document}

